{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "train_slip_detection_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![logo](https://neutouch.eu/templates/yootheme/cache/g37-01577415.png)\n",
        "# NeuTouch summer school on Touch and Robotics\n",
        "Tutorial: Slip Detection with Neural Networks\n",
        "\n",
        "In this tutorial we will look into incipient slip detection and slip classification with deep neural networks. We will experiment with and evaluate various neural network architectures to detect and classify slippage.\n",
        "To this end, we will work on a pre-recorded data set from our 16x16 tactile sensor array, comprising tactile time series data for three different situations:\n",
        "- stable grasp condition\n",
        "- translational slip\n",
        "- rotational slip\n"
      ],
      "metadata": {
        "id": "nAfMgIcOkP7C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Let's download the data set first:\n",
        "!wget \"https://uni-bielefeld.sciebo.de/s/7vi5lX9WO1VmvIZ/download\" -O \"data_stable_slip_rotate.pkl\""
      ],
      "outputs": [],
      "metadata": {
        "id": "o026L5tpXCTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94be0e6e-8b2c-4fe2-ee1b-3408565607bc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Actually load the data\n",
        "import pickle\n",
        "\n",
        "data = pickle.load(open(\"data_stable_slip_rotate.pkl\", \"rb\"), encoding='latin1')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ESKLclFdS2Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Visually inspect the data\n",
        "\n",
        "Use [matplotlib's animation](https://matplotlib.org/stable/api/animation_api.html) to create videos for all three classes (stable, translation, rotation)\n",
        "and [embed them into the notebook](http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-as-interactive-javascript-widgets/)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The data is organized into three classes:\n",
        "fig = plt.figure()  # create a new figure\n",
        "for idx, key in enumerate(data.keys()):\n",
        "    ax = fig.add_subplot(1, 3, idx+1, xticks=[], yticks=[],  # add subplot w/o any axis ticks\n",
        "                         xlabel=\"{name}: {shape}\".format(name=key, shape=data[key].shape))\n",
        "    ax.imshow(data[key][np.random.randint(0, len(data[key]))], cmap = \"Greys\")  # plot a random sample"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Z40x5j30ZIOD",
        "outputId": "ac316c7f-7761-45ba-fd4c-b263347df1e7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import random\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Limit video to a subset of the data\n",
        "subset = data['translation'][::100]\n",
        "\n",
        "# Set up the figure, the axis, and the plot element we want to animate\n",
        "fig = plt.figure(figsize=(2, 2))\n",
        "im = plt.imshow(subset[0], cmap='Greys')\n",
        "\n",
        "\n",
        "def animate_func(i):\n",
        "    im.set_array(subset[i])\n",
        "    return [im]\n",
        "\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate_func,\n",
        "                               frames=len(subset),\n",
        "                               interval=50,  # in ms\n",
        "                               )\n",
        "plt.close(fig)\n",
        "HTML(anim.to_html5_video())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline classification of raw data\n",
        "\n",
        "As a baseline approach, we illustrate here how to use a CNN to classify the raw data.\n",
        "As seen above, each class comprises 50k frames of consecutive sensor recordings, recorded at a frame rate of 1kHz. Hence, we have 50s of data for each class.\n",
        "Obviously, the network cannot predict from a single sample whether there is slippage or not. So, let's chop the whole time-series into short sequences of fixed length, say 32 samples, which then can be fed into a neural network."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import Keras + TensorFlow packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as keras\n",
        "\n",
        "# https://colab.research.google.com/notebooks/gpu.ipynb\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU:\", tf.test.gpu_device_name())\n",
        "\n",
        "# Define a random seed to have deterministic results\n",
        "np.random.seed(11)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HGOFo_hnWU66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Split data set into train and test\n",
        "def train_test_split(X, Y, ratio=0.8):\n",
        "   split=int(len(X)*ratio)\n",
        "   X_train = X[:split]\n",
        "   Y_train = Y[:split]\n",
        "\n",
        "   X_test = X[split:]\n",
        "   Y_test = Y[split:]\n",
        "\n",
        "   return X_train, X_test, Y_train, Y_test"
      ],
      "outputs": [],
      "metadata": {
        "id": "nHtfoUKxZAcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Split long time series into chunks of given length and stride\n",
        "def chop_time_series(series, length=32, stride=8):\n",
        "   # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator\n",
        "   N = len(series)\n",
        "   chunks, _ = keras.preprocessing.sequence.TimeseriesGenerator(\n",
        "      series, np.zeros(N), length=length, stride=stride, batch_size=N)[0]\n",
        "   # TimeseriesGenerator yields a format like (N, width, height)\n",
        "   # However TensorFlow uses channels_last as its default \"image\" format, i.e. (width, height, N)\n",
        "   # Thus, transpose into correct format:\n",
        "   return chunks.transpose(0, 2, 3, 1)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Prepare the dataset for training: chop into sequences of given length and assign numeric class labels\n",
        "def prepare_dataset(length=32, stride=8, ratio=0.8):\n",
        "   # map textual labels onto numeric class labels\n",
        "   label_mapping = dict(stable=0, translation=1, rotation=2)\n",
        "   x_train_test_y_train_test = []\n",
        "   for key in data.keys():\n",
        "      x = chop_time_series(data[key], length, stride)\n",
        "      y = np.full(len(x), label_mapping[key])  # generate numeric label for all xs\n",
        "      # optionally perform data augmentation (same on all elements of chunk!)\n",
        "      # optionally perform FFT on input data X\n",
        "      # https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html\n",
        "      print(key, x.shape)\n",
        "      # normalize 12bit ADC data (0..2^12) into range (0..1)\n",
        "      x_train_test_y_train_test.append(train_test_split(x / 4096, y, ratio=ratio))\n",
        "\n",
        "   # combine data from all classes into a single data set\n",
        "   return (np.concatenate([x_train_test_y_train_test[cls][sub] for cls in range(len(x_train_test_y_train_test))]) for sub in range(4))\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = prepare_dataset()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Simple MLP with a single hidden layer\n",
        "def simple_mlp(num_classes=3):\n",
        "   model = keras.models.Sequential(name=\"SimpleMLP\")\n",
        "   model.add(keras.layers.Flatten())  # Flatten input tensor into single vector\n",
        "   model.add(keras.layers.Dense(64, activation='relu'))  # hidden layer with 64 neurons\n",
        "   model.add(keras.layers.Dense(num_classes))  # dense classification layer\n",
        "   return model\n",
        "\n",
        "# Basic network with a single CNN layer\n",
        "def simple_cnn(num_classes=3):\n",
        "   model = keras.models.Sequential(name=\"SimpleCNN\")\n",
        "   model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
        "   model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "   model.add(keras.layers.Flatten())  # Flatten tensor into single vector\n",
        "   model.add(keras.layers.Dense(num_classes))  # dense classification layer\n",
        "   return model\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "v6jbsjEMZEop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf logs/"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train(model):\n",
        "   model.build(input_shape=X_train.shape)\n",
        "   model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=keras.optimizers.RMSprop(),\n",
        "                 metrics=['sparse_categorical_accuracy'])\n",
        "   model.summary()\n",
        "\n",
        "   # Specify log directory for TensorBoard\n",
        "   log_dir = \"logs/\" + model.name\n",
        "\n",
        "   # Initialize TensorBoard\n",
        "   tb = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "   # Fit data to the model!\n",
        "   model.fit(X_train, Y_train,\n",
        "             batch_size=16, epochs=10,\n",
        "             validation_data=(X_test, Y_test),\n",
        "             callbacks=[tb])\n",
        "\n",
        "   score = model.evaluate(X_test, Y_test, verbose=False)\n",
        "   print('Test loss:', score[0])\n",
        "   print('Test accuracy:', score[1])\n",
        "   return model"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "mlp = train(simple_mlp())\n",
        "cnn = train(simple_cnn())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Inspect results with TensorBoard\n",
        "%tensorboard --logdir logs"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Sanity check\n",
        "\n",
        "- The perfect classification accuracy is suspecious! Why is it so trivial for the networks to classify the data? Have a look at your data visualizations and compare the videos of the 3 classes!\n",
        "- To increase the variability of the data set, use [data augmentation techniques](https://www.tensorflow.org/tutorials/images/data_augmentation), e.g. to rotate the input data by 90°, 180°, 270° and shift by a few taxels.\n",
        "- As pointed out in the lecture, we are looking for micro vibrations. Thus introduce [FFT](https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html) preprocessing. Are frequencies and/or phases important?\n",
        "- Instead of considering the whole 16x16 array as input, we could split the array into smaller units, e.g. 2x2 patches. This will make the problem harder, because the network needs to recognize slippage from those smaller patches. On the other hand, we also increase the dataset. Ensure that you only include patches that actually have contact!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Experiment!\n",
        "Try various networks:\n",
        "- Add more CNN layers\n",
        "- Adapt size of layers, size of kernels, num of kernels\n",
        "- Experiment with different optimizers: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "\n",
        "Improve generalization\n",
        "- Introduce drop-out: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
        "- Use batch normalization: https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"
      ],
      "metadata": {}
    }
  ]
}